{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SWSN\\swsn_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from Encoders import *\n",
    "from transformers import ElectraTokenizer, ElectraModel, DebertaV2Tokenizer, DebertaV2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "(20,)\n",
      "(384,)\n",
      "(384,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(512,)\n",
      "(768,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n",
      "(512,)\n",
      "(768,)\n",
      "(1024,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Hello World!\"\n",
    "\n",
    "models_different_size = [\n",
    "    TextEncoder(NormalEncoder((20,), mean=0, std=1), target_dim=None),\n",
    "    TextEncoder(DummyEncoder((20, )), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/all-MiniLM-L6-v2\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/all-MiniLM-L12-v2\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/all-mpnet-base-v2\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/bert-base-nli-mean-tokens\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/roberta-base-nli-mean-tokens\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens\"), target_dim=None),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/distiluse-base-multilingual-cased-v2\"), target_dim=None),\n",
    "    TextEncoder(GPT_2Encoder(), target_dim=None),\n",
    "    TextEncoder(OpenAIEncoder(\"text-embedding-3-small\"), target_dim=None),\n",
    "    TextEncoder(T5Encoder(), target_dim=None),\n",
    "    TextEncoder(LongformBase4096(), target_dim=None),\n",
    "    TextEncoder(HiddenStateTransformer(DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-large\"), DebertaV2Model.from_pretrained(\"microsoft/deberta-v3-large\")), target_dim=None),\n",
    "    TextEncoder(HiddenStateTransformer(ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\"), ElectraModel.from_pretrained(\"google/electra-base-discriminator\")), target_dim=None),\n",
    "]\n",
    "\n",
    "for model in models_different_size:\n",
    "    encoding = model.encode(test_text)\n",
    "    print(encoding.shape)\n",
    "    assert encoding is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Hello World!\"\n",
    "\n",
    "models_same_size = [\n",
    "    TextEncoder(NormalEncoder((20,), mean=0, std=1), target_dim=1024),\n",
    "    TextEncoder(DummyEncoder((20, )), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/all-MiniLM-L6-v2\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/all-MiniLM-L12-v2\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/all-mpnet-base-v2\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/bert-base-nli-mean-tokens\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/roberta-base-nli-mean-tokens\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens\"), target_dim=1024),\n",
    "    TextEncoder(SentenceTransformerEncoder(\"sentence-transformers/distiluse-base-multilingual-cased-v2\"), target_dim=1024),\n",
    "    TextEncoder(GPT_2Encoder(), target_dim=1024),\n",
    "    TextEncoder(OpenAIEncoder(\"text-embedding-3-small\"), target_dim=1024),\n",
    "    TextEncoder(T5Encoder(), target_dim=1024),\n",
    "    TextEncoder(LongformBase4096(), target_dim=1024),\n",
    "    TextEncoder(HiddenStateTransformer(DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-large\"), DebertaV2Model.from_pretrained(\"microsoft/deberta-v3-large\")), target_dim=1024),\n",
    "    TextEncoder(HiddenStateTransformer(ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\"), ElectraModel.from_pretrained(\"google/electra-base-discriminator\")), target_dim=1024),\n",
    "]\n",
    "\n",
    "for model in models_same_size:\n",
    "    encoding = model.encode(test_text)\n",
    "    print(encoding.shape)\n",
    "    assert encoding is not None\n",
    "    assert len(encoding) == 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swsn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
